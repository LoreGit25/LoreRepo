# 系统设计面试指南（LLM部署与架构方向）

系统设计面试在 LLM Engineer 与 Infra/Platform 方向中日益重要，尤其在以下领域：大模型推理服务部署、RLHF流水线构建、多模型调度、Agent系统集成等。

面试官关注的不仅是“你懂技术”，更是“你能不能搭建一个稳定、可扩展的真实系统”。

---

## 🧱 一、常见系统设计题型

| 类型 | 示例问题 | 考察重点 |
|------|----------|----------|
| 推理服务设计 | 如何设计一个高并发的 ChatGPT 服务？ | 延迟优化、KV Cache、请求调度 |
| 微调平台设计 | 如何构建一个支持 LoRA/SFT 的训练系统？ | 分布式训练、数据管理、日志追踪 |
| 多模型调度系统 | 如何支持不同任务快速切换 LLM？ | 模型路由、多模型加载/卸载、内存隔离 |
| RLHF Pipeline | 如何调度 SFT → RM → PPO？ | 阶段任务管理、数据依赖、资源控制 |
| Agent 系统设计 | 如何构建一个支持工具调用、记忆的 LLM Agent？ | 模块交互、API调用、推理顺序与状态管理 |

---

## 🧩 二、通用答题结构（建议模板）

### 🧭 Step-by-step 模板

1. **Clarify Requirements**
   - 是训练系统还是推理服务？
   - 实时 vs 离线？吞吐 vs 延迟？
   - 用户数、模型大小、请求模式？

2. **High-Level Architecture**
   - 拆分为模块（API → Cache → Model → Log）
   - 数据流路径（用户输入 → Tokenizer → Inference → Response）

3. **Component Design**
   - 模型加载机制（热加载 / 多GPU管理）
   - 输入预处理、Prompt拼接、Token限制等细节
   - Batch调度策略：FIFO / 优先级调度 / Token Alignment

4. **Performance Optimization**
   - KV Cache 共享机制
   - Streaming & Speculative Decoding
   - 模型压缩（INT4、LoRA）+ 混合精度

5. **Observability & Scalability**
   - Prometheus / Grafana 监控指标（QPS、延迟、OOM）
   - 模型版本管理、自动扩容、AB测试支持

---

## ⚙️ 三、经典示例题解析

### 🎯 示例 1：高并发 LLM 推理服务

> 设计一个多用户、多轮对话的推理服务系统，支持实时响应与上下文保留。

**核心模块**：

- FastAPI + Queue + vLLM 后端，支持 Streaming + Chat History
- 使用 KV Cache 支持多轮上下文拼接
- 提供 async API 接口，token level streaming 回传
- 限流器 + Token Limiter 保证资源公平使用

**加分点**：
- 使用 Triton 统一接入多个模型（LLaMA, Qwen）
- 将模型冷启动时间缩短至秒级（提前预加载 + GPU探活）

---

### 🎯 示例 2：RLHF Pipeline 训练平台

> 如何设计支持 SFT → RM → PPO 的 RLHF 训练系统？

**模块划分**：

1. 数据管理模块（统一格式，支持 prompt pair / ranking 结构）
2. SFT 模块（LoRA/FSDP + checkpoint 存储）
3. RM 训练模块（支持动态打分导入）
4. PPO 优化器（KL 控制 + reward + reference 模型管理）
5. 实验追踪（wandb / TensorBoard）

**系统特性**：

- 支持中断恢复（断点训练）
- 每个阶段可独立执行或级联调度（workflow 调度器）

---

## 🧠 四、工程能力展示建议（面试中如何“秀肌肉”）

| 能力方向 | 展示方式 |
|----------|----------|
| 模块理解力 | 清晰拆解每个子系统的数据流、计算流程 |
| 性能意识 | 主动提 KV Cache、并发优化、批处理策略 |
| 异常处理 | 提及 Timeout、TokenOverflow、OOM 保护机制 |
| 可扩展性 | 提到如何横向扩容、分区加载模型、支持 A/B Test |
| 实践经验 | 用“我在项目中如何做过”加深说服力

---

## 💬 五、常见 Follow-up 面试问题

- 你如何避免 KV Cache 占用过大？
- 多轮对话中如何安全保存用户上下文？
- 如果 GPU 资源不足，如何调度多个模型？
- PPO 阶段经常 loss 不下降，你怎么看？
- 你的系统如何支持热更新模型？

---

## ✅ 六、项目经验话术模板（可用于简历 or 面试）

- “构建了基于 vLLM 的高并发推理服务，支持多用户异步访问与 Streaming 响应，平均响应时间降低 45%。”
- “设计了 RLHF 三阶段流水线，集成 DeepSpeed + Axolotl，实验支持中断恢复与动态参数注入。”
- “搭建内网多模型微服务系统，实现 LLaMA、Qwen、Baichuan 的动态加载和调度。”

---

## 🧰 七、工具清单（系统设计常用技术栈）

| 类别 | 工具 |
|------|------|
| 推理框架 | vLLM, Triton Server, Hugging Face Transformers |
| 分布式训练 | DeepSpeed, FSDP, Megatron-LM |
| Agent工具 | LangChain, CrewAI, AutoGPT |
| 服务部署 | FastAPI, gRPC, Docker, Kubernetes |
| 可视化监控 | Weights & Biases, Prometheus + Grafana |
| 数据流管理 | MinIO, Redis, MongoDB（上下文状态 / 评分日志）

---

📁 返回上级：[简历与面试准备.md](../简历与面试准备.md)
