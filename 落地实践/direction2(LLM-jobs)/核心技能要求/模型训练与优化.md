# 模型训练与优化（LLM核心能力）

大语言模型的训练是一个涉及海量数据、计算资源和精细优化策略的系统工程。本章介绍 LLM 的核心训练技术，包括预训练、指令微调、奖励建模、RLHF、多卡训练与调参方法等，是求职与面试的重点板块。

---

## 📚 一、训练阶段全景图

大模型通常采用“分阶段训练”范式：

| 阶段 | 目标 | 典型技术 |
|------|------|----------|
| **预训练（Pretraining）** | 获取通用语言理解能力 | 自监督：Causal LM / MLM / Prefix LM |
| **指令微调（SFT）** | 对齐指令风格与用户意图 | 基于 prompt-response pairs 监督学习 |
| **奖励建模（RM）** | 建立人类偏好评分模型 | 用 ranking 数据训练二分类/排序网络 |
| **强化学习优化（RLHF）** | 进一步行为控制、优化输出质量 | PPO（Proximal Policy Optimization）/ DPO |

### 🎯 每一阶段你应掌握的核心技能

- **预训练阶段**：理解 Tokenizer → DataLoader → loss → checkpoint 流程
- **SFT阶段**：搭建指令式训练框架，支持多个模板、padding处理、混合精度
- **RM训练**：实现 pairwise ranking loss，处理奖励数据清洗
- **RLHF阶段**：理解 KL 控制、参考模型冻结、经验回放机制

---

## 🏗️ 二、分布式训练与工程优化

### 1. 模型并行策略对比

| 并行类型 | 框架 | 场景适用 |
|----------|------|----------|
| Data Parallelism | PyTorch DDP | 适用于中小模型 / 单节点多GPU |
| Tensor Parallelism | Megatron-LM, DeepSpeed | 大模型计算拆分（矩阵维度切分） |
| Pipeline Parallelism | Megatron-LM | 长模型结构按层切分（适合Transformer） |
| ZeRO (Stage 1/2/3) | DeepSpeed | 参数/梯度/优化器状态切分以节省显存 |
| Fully Sharded (FSDP) | PyTorch 官方 | 简洁、稳定，支持混合精度和重计算 |

> 推荐初学者从 FSDP + Hugging Face Accelerate 入手，逐步扩展到 DeepSpeed + ZeRO-3。

---

## ⚙️ 三、微调技巧与参数高效训练

### 1. LoRA / QLoRA

| 方法 | 原理 | 应用场景 |
|------|------|----------|
| **LoRA** | 注入低秩矩阵到权重层进行训练 | 减少训练参数，仅训练 A/B 矩阵 |
| **QLoRA** | 配合 4bit量化权重，节省显存训练 | 可在单张 24G 显存卡上训练 7B 模型 |

- 推荐工具：[PEFT](https://github.com/huggingface/peft), [QLoRA Template](https://github.com/artidoro/qlora)

### 2. Chat 模型微调注意事项
- 使用 ChatTemplate（如 ChatML / Alpaca style）统一指令格式
- 注意 BOS/EOS token，padding 位置与 loss mask 区分
- 多样 prompt 模板能显著提升 generalization 能力

---

## 🧪 四、调参策略与优化技巧

### 1. 超参数范围与解释

| 参数 | 建议范围 | 说明 |
|--------|-----------|----------------------------|
| learning rate | 1e-5 ~ 1e-4 | linear warmup + cosine decay |
| batch size | 按 GPU 显存调节 | 太小会导致训练不稳定 |
| warmup steps | 500 ~ 3000 | 避免初期 loss 爆炸 |
| gradient clipping | 1.0 通用 | 防止梯度爆炸，PPO 中尤其重要 |
| weight decay | 0.01 ~ 0.1 | 控制正则化，避免过拟合 |

### 2. 优化器对比

| 优化器 | 特点 |
|--------|------|
| AdamW | 业界默认，高鲁棒性 |
| LAMB | 大 batch 高效，预训练常用 |
| Adafactor | 节省内存，适合参数特别大的模型 |

---

## 📈 五、模型评估与实验追踪

### 1. 评估指标（依任务类型）

| 任务 | 常用指标 |
|------|----------|
| 分类 / RM | Accuracy, AUC, Kendall Tau |
| 生成任务 | BLEU, ROUGE, chrF, BERTScore |
| 对话 / 多轮问答 | Win-rate, G-Eval, MT-Bench |
| 对齐性 | Toxicity Score, Harmlessness Score, TruthfulQA |

### 2. 实验追踪工具

| 工具 | 优势 |
|------|------|
| Weights & Biases | 最流行、支持超参扫表、可视化比较 |
| TensorBoard | 快速上手，适合单机本地实验 |
| ClearML / MLflow | 支持实验版本管理，适合多人协作 |

---

## 🧯 六、常见问题排查清单（Debug Checklist）

| 问题 | 常见原因 | 建议排查方式 |
|------|----------|--------------|
| `loss 为 NaN` | 学习率太高、token mask错、label错位 | 查看学习率/gradient分布，打印input+label样本 |
| 模型无收敛 | 数据质量差、未shuffle | 检查 tokenizer、数据覆盖范围 |
| 显存溢出 | batch太大、未关闭grad | 启用 gradient checkpoint、设置 max_memory_map |
| 微调没效果 | 模板设计不佳、label偏移 | 用few-shot example debug，人工评估输出 |

---

## 🧪 七、实战路径建议（练习路线）

| 阶段 | 实践任务 |
|------|----------|
| Step 1 | 使用 `Transformers` 训练一个分类或生成模型 |
| Step 2 | 用 `LoRA` 在 Alpaca-style 指令数据上微调 LLaMA-7B |
| Step 3 | 构建 SFT → RM → PPO 的完整 RLHF 流程（用 Axolotl / TRL） |
| Step 4 | 利用 `DeepSpeed + wandb` 训练大模型并记录多次实验差异 |
| Step 5 | 尝试调参 sweep / early stopping / 模型剪枝提升训练效率 |

---

## 🎯 面试常见问题举例（适合工程 / 科研岗位）

- 什么是 gradient checkpointing？它如何节省显存？
- LoRA 和 Adapter 微调的本质差别是什么？
- PPO中 KL 控制的意义是什么？为什么要用参考模型？
- 你如何设计一个 SFT pipeline 支持多模态或多个 prompt 模板？
- 多机训练中如何处理 parameter sync 和 loss scale？

---

📁 返回上级：[核心技能要求.md](../核心技能要求.md)
