# 技术面试准备指南（LLM岗位专属）

LLM 相关岗位的技术面试，既考察算法与编程能力，又强调候选人对大模型训练原理、分布式工程系统、推理优化、微调机制等方向的深入理解与实操经验。

---

## 🧪 一、典型面试流程结构

| 阶段 | 内容 | 说明 |
|------|------|------|
| 初筛 | 电话面试 / 在线编程 | 算法题 + 项目经验 + 简单建模问题 |
| 技术轮 | 多轮深度面试 | 训练流程 + 推理优化 + 系统设计 + 项目细节挖掘 |
| 最终轮 | Team Match / Hiring Committee | 技术匹配度 + 沟通表达 + 跨团队协作能力评估 |

---

## 🧠 二、核心知识模块与考察重点

### 1. 算法与编程基础（尤其适用于 Engineer / Applied Scientist）

- 高频考点：
  - 数组滑窗 / 栈队列模拟（token buffer、streaming memory）
  - 字符串操作（Prompt 拼接、正则匹配）
  - 图搜索（多 Agent 系统推理、Action Planning）
  - 动态规划（序列解码、多任务组合）
- 建议平台：
  - LeetCode：面试 Top100、Mid题难度为主
  - Codeforces / AtCoder：锻炼代码速度与准确性

### 2. 模型机制与训练流程

- Transformer：Attention, Positional Embedding, Residual Block
- 模型训练三阶段：
  - SFT：Prompt 格式设计、mask 处理、loss 监督位置
  - Reward Model：ranking loss 设计、数据打分方式（pairwise vs listwise）
  - RLHF：PPO vs DPO，KL 控制，参考模型冻结
- 微调方法对比：Full / LoRA / QLoRA / Adapter

### 3. 分布式与推理系统设计

- FSDP / DeepSpeed / ZeRO-3 分布式训练原理与适用场景
- KV Cache 设计与复用、Prefill & Decode 优化策略
- 推理加速框架（vLLM, ONNX, TensorRT）核心原理
- Prompt Pipeline（多工具调用、多模板组合）的处理方式

### 4. Debugging 与优化实战能力

| 问题类型 | 举例 |
|----------|------|
| loss 异常 | “训练loss为NaN，你如何排查？” |
| 显存超限 | “如何优化一个 13B 模型在单机上微调？” |
| 收敛缓慢 | “怎样调节 learning rate / batch size 提升收敛效率？” |
| token生成偏差 | “生成结果重复、无意义时你会检查什么？” |

---

## 🧩 三、项目讲解结构模板 + 应答建议

结构化讲解项目是面试中最核心、最有效展示自身能力的方式。

### 推荐模板 STAR+IRR（实际可套入任何项目）：

| 模块 | 建议内容 |
|------|----------|
| 背景（S） | 项目场景、问题来源（如：公司内部对话助手 / 开源竞赛） |
| 目标（T） | 解决的问题：如部署 7B 模型、优化 response 质量 |
| 行动（A） | 用了哪些技术：如 LoRA + ChatTemplate + DeepSpeed |
| 结果（R） | 定量指标：如 BLEU +15%，推理延迟 -60% |
| 创新点（I） | 自己设计的关键逻辑、相较 SOTA 差异点 |
| 扩展思考（R） | 遇到的挑战、部署问题、未来改进方向

---

## 🧰 四、建议准备材料与技巧

| 材料 | 建议内容 |
|------|----------|
| 📌 项目图 / 模块图 | 绘制模型结构或 pipeline，辅助说明复杂系统 |
| 📁 GitHub 项目文档 | README + 目录结构 + 示例代码段 |
| 🧾 SFT / RLHF 流程伪代码 | 能手写三阶段训练伪代码（显示理解流程） |
| 📊 参数表 / 结果表格 | 用于展示优化路径，如 sweep 结果对比图 |
| 🎯 两套项目讲解 | 一套偏工程（如推理优化），一套偏研究（如新算法） |

---

## 💬 五、面试题样例汇总（实际题）

### 模型机制类
- Q: “请解释一下 Attention 的计算过程”
- Q: “LoRA 中的 A/B 矩阵是否共享？训练时更新哪些参数？”

### 训练系统类
- Q: “你会如何构建一个支持多阶段训练的 pipeline？”
- Q: “如果我们要从 base GPT-2 训练出一个 Q&A 模型，你的步骤会是？”

### 推理优化类
- Q: “如何用 KV Cache 提升推理速度？哪些 token 会重新计算？”
- Q: “你熟悉 vLLM 吗？它和普通推理方式有何不同？”

### Debug / 实操类
- Q: “训练 loss 波动很大，你会怎么调参？”
- Q: “显存不够跑 13B 模型，你有哪些策略？”

---

## 💡 六、面试前的高效准备路径

| 时间点 | 建议内容 |
|--------|----------|
| T-14 天 | 阅读 LLaMA / GPT / LoRA / RLHF 等核心论文 |
| T-10 天 | 用 Hugging Face 或 Axolotl 跑一遍微调流程 |
| T-7 天 | 手写项目讲解文档，准备 STAR 模板回答 |
| T-3 天 | 与同学 / 模拟面试官进行模拟演练 |
| 面试当天 | 自信表达，注重逻辑与 impact，问题不会时主动沟通思路即可 |

---

## 🚫 常见误区提醒

- ❌ 只讲项目“做了什么”，忽略“为什么这么做”与“你在其中的作用”
- ❌ 忽视基础概念，像“mask的作用”、“Loss位置处理”讲不清
- ❌ 不会画结构图、图文表达项目 pipeline
- ❌ 面对 RLHF / PPO 等关键词说“听说过但没深入”

---

📁 返回上级：[简历与面试准备.md](../简历与面试准备.md)
